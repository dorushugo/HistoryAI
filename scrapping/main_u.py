import subprocess
import pandas as pd
from scrappers.url_detail import get_infos_guerre  # Importation de la fonction pour rÃ©cupÃ©rer les infos dÃ©taillÃ©es

def main():
    print("DÃ©marrage du script principal")
    # Ton code principal ici

    # Ã‰tape 1: RÃ©cupÃ©rer les URLs des guerres via le premier scraper
    print("ğŸ“¡ RÃ©cupÃ©ration des URLs des guerres...")
    subprocess.run(["python3", "scrappers/scrapper_url.py"])

    # Ã‰tape 2: Charger les URLs rÃ©cupÃ©rÃ©es dans le fichier CSV
    guerres_df = pd.read_csv("details_guerres.csv")

    guerres_details = []

    # Ã‰tape 3: RÃ©cupÃ©rer les informations dÃ©taillÃ©es pour chaque guerre
    print("ğŸ“ RÃ©cupÃ©ration des informations dÃ©taillÃ©es...")
    for index, row in guerres_df.iterrows():
        print(f"ğŸ“ RÃ©cupÃ©ration des informations pour {row['Nom']}...")
        infos_guerre = get_infos_guerre(row["URL"])
        if infos_guerre:
            guerres_details.append(infos_guerre)

    # Ã‰tape 4: Sauvegarder les informations dÃ©taillÃ©es dans un CSV
    details_df = pd.DataFrame(guerres_details)
    details_df.to_csv("guerres_details_completes.csv", index=False, encoding="utf-8")
    print("âœ… DonnÃ©es dÃ©taillÃ©es enregistrÃ©es dans 'guerres_details_completes.csv'.")

if __name__ == "__main__":
    main()