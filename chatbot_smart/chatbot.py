from fastapi import FastAPI
from pydantic import BaseModel
import ollama
import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import joblib

# ğŸ”¹ Charger le modÃ¨le et le vectorizer entraÃ®nÃ©s
vectorizer = joblib.load("vectorizer.pkl")
classifier = joblib.load("intent_classifier.pkl")

def detect_intent(message):
    """ DÃ©tecte lâ€™intention de lâ€™utilisateur """
    X = vectorizer.transform([message])
    print(classifier.predict(X)[0])
    return classifier.predict(X)[0]
    
app = FastAPI()

# ğŸ“Œ DÃ©finir le modÃ¨le Pydantic pour la requÃªte
class ChatRequest(BaseModel):
    user_id: str
    message: str

# ğŸ”¹ Charger FAISS et le modÃ¨le d'embeddings
index = faiss.read_index("data/guerres_faiss_NLP.index")
df_events = pd.read_pickle("data/guerres_mapping_NLP.pkl")
# embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
embedding_model = SentenceTransformer('Lajavaness/bilingual-document-embedding', trust_remote_code=True)

quiz_sessions = {}
conversation_memory = {}

def generate_quiz(topic):
    """ GÃ©nÃ¨re un quizz Ã  partir de la base de donnÃ©es vectorielle ou indique que le sujet est inconnu """
    global quiz_sessions
    global conversation_memory
    user_id ="123"

        # âœ… CrÃ©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # âœ… VÃ©rifier que chaque Ã©lÃ©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("âš ï¸ Erreur : `conversation_memory` contient des donnÃ©es incorrectes ! RÃ©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # ğŸ”¹ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)

    # ğŸ” 1ï¸âƒ£ Recherche du sujet dans la base vectorielle FAISS
    query_embedding = embedding_model.encode([topic])
    D, I = index.search(np.array(query_embedding), k=1)  # RÃ©cupÃ©rer le meilleur rÃ©sultat
    best_match_id = I[0][0]
    confidence = D[0][0]

    SEUIL_CONFIANCE_BAS = 5  # En dessous, on dit qu'on ne connaÃ®t pas
    SEUIL_CONFIANCE_LLM = 2  # Entre 5 et 10, on peut utiliser le LLM pour complÃ©ter

    if confidence < SEUIL_CONFIANCE_LLM:
        # ğŸ“Œ RÃ©cupÃ©rer les informations du sujet
        event = df_events.iloc[best_match_id]
        contexte = (
            f"ğŸ“œ **{event['Nom']}**\n"
            f"ğŸ“… **Date** : {event['Date']}\n"
            f"ğŸ“ **Lieu** : {event['Lieu']}\n"
            f"ğŸ“ **RÃ©sumÃ©** : {event['RÃ©sumÃ©']}\n"
        )
        # ğŸ¯ 2ï¸âƒ£ GÃ©nÃ©rer un quizz basÃ© sur ces informations
        llm_prompt = f"""
        Voici l'historique de la conversation avec l'utilisateur :
        {conversation_text}
        
        Mets-toi dans la peau d'un assistant pour les rÃ©visions.
        Utilise les informations suivantes pour crÃ©er un quizz sur {topic}.
        Donne 5 questions Ã  choix multiples pertinentes sans donner les rÃ©ponses.

        **Informations :**
        {contexte}
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
        llm_quiz = response["message"]["content"]

        # ğŸ“Œ Sauvegarde des questions gÃ©nÃ©rÃ©es
        quiz_sessions[user_id] = [(q.strip(), "??") for q in llm_quiz.split("\n") if q.strip()]
        print('ğŸ“‹ Quizz gÃ©nÃ©rÃ© avec les donnÃ©es disponibles :\n\n')
        return f"{llm_quiz}"

    elif confidence > SEUIL_CONFIANCE_LLM and confidence < SEUIL_CONFIANCE_BAS:
        # ğŸ“Œ 3ï¸âƒ£ Confiance trop faible â†’ On utilise le LLM sans source vÃ©rifiÃ©e
        llm_prompt = f"""
        Mets-toi dans la peau d'un assistant pour les rÃ©visions.
        GÃ©nÃ¨re un quizz sur {topic}.
        Donne 5 questions Ã  choix multiples pertinentes sans donner les rÃ©ponses.
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
        llm_quiz = response["message"]["content"]

        # ğŸ“Œ Sauvegarde temporaire des questions gÃ©nÃ©rÃ©es (sans rÃ©ponses)
        quiz_sessions[user_id] = [(q.strip(), "??") for q in llm_quiz.split("\n") if q.strip()]
        print("ğŸ“‹ Quizz gÃ©nÃ©rÃ© par l'IA (donnÃ©es incertaines) :\n\n")
        return f"{llm_quiz}"

    else:
        # ğŸ“Œ 4ï¸âƒ£ Confiance trop basse â†’ On refuse de rÃ©pondre
        return "âš ï¸ DÃ©solÃ©, je nâ€™ai pas assez dâ€™informations pour gÃ©nÃ©rer un quizz sur ce sujet."

def provide_quiz_answers(user_id):
    """ Renvoie les rÃ©ponses du dernier quizz demandÃ© par l'utilisateur, si disponible """

    global quiz_sessions  # ğŸ“Œ Permet d'accÃ©der Ã  la variable globale
    global conversation_memory
        # âœ… CrÃ©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # âœ… VÃ©rifier que chaque Ã©lÃ©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("âš ï¸ Erreur : `conversation_memory` contient des donnÃ©es incorrectes ! RÃ©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # ğŸ”¹ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)

    if user_id not in quiz_sessions:
        return "âš ï¸ Aucun quizz en cours. Demandez-moi dâ€™en gÃ©nÃ©rer un !"

    # ğŸ“Œ RÃ©cupÃ©rer les questions stockÃ©es
    questions = quiz_sessions[user_id]

    # ğŸ¯ VÃ©rifier si on a dÃ©jÃ  les rÃ©ponses
    if "??" not in [answer for _, answer in questions]:
        return "\n".join([f"âœ… {q} : **{a}**" for q, a in questions])

    # ğŸ¯ Si les rÃ©ponses sont inconnues, demander au LLM
    print("âš ï¸ RÃ©ponses manquantes, gÃ©nÃ©ration par LLM...")

    llm_prompt = f"""
    Voici l'historique de la conversation avec l'utilisateur :
    {conversation_text}

    Voici un quizz d'histoire :
    {chr(10).join([q for q, _ in questions])}

    Donne une rÃ©ponse prÃ©cise pour chaque question sous forme de liste numÃ©rotÃ©e.
    """

    response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
    llm_answers = response["message"]["content"].split("\n")

    # ğŸ“Œ Mettre Ã  jour les rÃ©ponses dans la session
    for i, (q, _) in enumerate(questions):
        if i < len(llm_answers):
            quiz_sessions[user_id][i] = (q, llm_answers[i].strip())
    print("ğŸ“‹ RÃ©ponses du quizz :\n\n")
    return f"{chr(10).join(llm_answers)}"

def generate_summary(topic):
    """ GÃ©nÃ¨re un rÃ©sumÃ© simple et pÃ©dagogique pour aider un Ã©lÃ¨ve Ã  rÃ©viser """
    global conversation_memory
    user_id ="123"

    # âœ… CrÃ©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # âœ… VÃ©rifier que chaque Ã©lÃ©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("âš ï¸ Erreur : `conversation_memory` contient des donnÃ©es incorrectes ! RÃ©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # ğŸ”¹ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)
    # ğŸ” 1ï¸âƒ£ Recherche de l'Ã©vÃ©nement dans FAISS
    query_embedding = embedding_model.encode([topic])
    D, I = index.search(np.array(query_embedding), k=1)
    best_match_id = I[0][0]
    confidence = D[0][0]
    print(confidence)
    SEUIL_CONFIANCE_BAS = 5

    if confidence < SEUIL_CONFIANCE_BAS:
        event = df_events.iloc[best_match_id]
        contexte = f"ğŸ“œ **{event['Nom']}**\nğŸ“… **Date** : {event['Date']}\nğŸ“ **Lieu** : {event['Lieu']}\nğŸ“ **RÃ©sumÃ©** : {event['RÃ©sumÃ©']}"

        # ğŸ¯ Demander au LLM un rÃ©sumÃ© pour un Ã©lÃ¨ve
        llm_prompt = f"""
        Voici l'historique de la conversation avec l'utilisateur :
        {conversation_text}

        Tu es un professeur d'histoire. RÃ©dige un rÃ©sumÃ© pÃ©dagogique sur {event['Nom']} destinÃ© Ã  un Ã©lÃ¨ve de lycÃ©e. 
        Utilise ces informations :
        {contexte}

        Explique de maniÃ¨re simple et claire, en 150 mots maximum.
        Ne parle pas Ã  l'Ã©lÃ¨ve, donne-lui simplement le rÃ©sumÃ©
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
        print(f"ğŸ“œ **RÃ©sumÃ© pÃ©dagogique de {event['Nom']}** :\n\n")
        return f"{response['message']['content']}"

    return "âš ï¸ DÃ©solÃ©, je nâ€™ai pas assez dâ€™informations pour gÃ©nÃ©rer un rÃ©sumÃ© sur ce sujet."

def search_history(message):
    """ Recherche un Ã©vÃ©nement historique et l'explique en dÃ©tail pour un Ã©lÃ¨ve """
    global conversation_memory
    user_id ="123"

    # âœ… CrÃ©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # âœ… VÃ©rifier que chaque Ã©lÃ©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("âš ï¸ Erreur : `conversation_memory` contient des donnÃ©es incorrectes ! RÃ©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # ğŸ”¹ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)

    # ğŸ” GÃ©nÃ©rer l'embedding de la requÃªte
    query_embedding = embedding_model.encode([message])
    D, I = index.search(np.array(query_embedding), k=1)
    best_match_id = I[0][0]
    confidence = D[0][0]
    print(confidence)
    SEUIL_CONFIANCE_BAS = 5

    if confidence > SEUIL_CONFIANCE_BAS:
        return "âš ï¸ DÃ©solÃ©, je nâ€™ai pas trouvÃ© dâ€™informations prÃ©cises sur ce sujet."

    event = df_events.iloc[best_match_id]
    contexte = f"ğŸ“œ **{event['Nom']}**\nğŸ“… **Date** : {event['Date']}\nğŸ“ **Lieu** : {event['Lieu']}\nğŸ“ **RÃ©sumÃ©** : {event['RÃ©sumÃ©']}"
    print(contexte)
    # ğŸ¯ Demander au LLM d'expliquer le sujet en dÃ©tail
    llm_prompt = f"""
    Voici l'historique de la conversation avec l'utilisateur :
    {conversation_text}

    Explique Ã  un Ã©lÃ¨ve de lycÃ©e l'Ã©vÃ©nement historique suivant :
    {contexte}

    RÃ©dige une explication dÃ©taillÃ©e et pÃ©dagogique en 300 mots maximum.
    Ne parle pas Ã  l'Ã©lÃ¨ve, donne-lui simplement l'explication dÃ©taillÃ©e

    """
    # Structure la rÃ©ponse en introduction, dÃ©veloppement et conclusion.

    response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
    print(f"ğŸ“œ **Explication dÃ©taillÃ©e de {event['Nom']}** :\n\n")
    return f"{response['message']['content']}"

@app.post("/chat")
def chat(request: ChatRequest):
    """ GÃ¨re la conversation et dÃ©tecte lâ€™intention automatiquement """

    user_id = request.user_id
    message = request.message.lower()

    # ğŸ¯ 1ï¸âƒ£ VÃ©rifier que l'utilisateur a une liste d'historique
    if user_id not in conversation_memory:
        conversation_memory[user_id] = []  # ğŸ“Œ CrÃ©er une liste vide pour ce user
    conversation_history = conversation_memory[user_id]

    # âœ… Enregistrer le message de l'utilisateur dans l'historique AVANT toute analyse
    conversation_history.append({"role": "user", "content": message})

    # ğŸ¯ 1ï¸âƒ£ DÃ©tecter lâ€™intention de la requÃªte
    intent = detect_intent(message)

    print(f"ğŸ“© Message reÃ§u : {message} â†’ Intention dÃ©tectÃ©e : {intent}")

    # ğŸ¯ 2ï¸âƒ£ Rediriger vers la bonne fonction
    if intent == "quizz":
        response_text = generate_quiz(message)
    elif intent == "quizz_reponses":
        response_text = provide_quiz_answers(user_id)
    elif intent == "rÃ©sumÃ©":
        response_text = generate_summary(message)
    elif intent == "dÃ©tail":
        response_text = search_history(message)
    else:
        response_text = "âš ï¸ Je ne suis pas sÃ»r de comprendre. Peux-tu reformuler ?"
    # ğŸ“Œ Stocker la rÃ©ponse dans la mÃ©moire
    conversation_history.append({"role": "assistant", "content": response_text})
    return {"response": response_text}





# import numpy as np
# import pandas as pd

# # ğŸ”¹ Charger FAISS et le modÃ¨le d'embeddings
# index = faiss.read_index("data/guerres_faiss_NLP.index")
# df_events = pd.read_pickle("data/guerres_mapping_NLP.pkl")
# embedding_model = SentenceTransformer('Lajavaness/bilingual-document-embedding', trust_remote_code=True)

# # ğŸ”¹ Liste de requÃªtes historiques
# test_messages = [
#     "Guerre de la Ligue de Cambrai", "Guerre de Trente Ans", "Guerre de DÃ©volution",
#     "Guerre de Hollande", "Guerre de la Ligue d'Augsbourg", "Guerre de Succession d'Espagne",
#     "Guerre de la Quadruple-Alliance", "Guerre de Succession de Pologne", "Guerre de Succession d'Autriche",
#     "Guerre de Sept Ans", "Guerre d'IndÃ©pendance des Ã‰tats-Unis", "Guerres de la RÃ©volution franÃ§aise",
#     "Guerres napolÃ©oniennes", "Guerre de 1812", "Guerre d'indÃ©pendance grecque",
#     "Guerre de CrimÃ©e", "Guerre de SÃ©cession", "Guerre franco-prussienne",
#     "Guerre des Boers", "PremiÃ¨re Guerre mondiale", "Guerre civile russe",
#     "Guerre d'indÃ©pendance irlandaise", "Guerre civile chinoise", "Guerre d'Espagne",
#     "Seconde Guerre mondiale", "Guerre de CorÃ©e", "Guerre d'AlgÃ©rie",
#     "Guerre du Vietnam", "Guerre du Yom Kippour", "Guerre d'Afghanistan",
#     "Guerre Iran-Irak", "Guerre du Golfe", "Guerre de Bosnie",
#     "Guerre du Kosovo", "Guerre d'Irak", "Guerre civile syrienne",
#     "Guerre du Donbass", "Guerre du Haut-Karabagh", "Guerre civile libyenne",
#     "Guerre civile yÃ©mÃ©nite", "Guerre du Mali", "Guerre civile centrafricaine",
#     "Guerre du Darfour", "Guerre civile sud-soudanaise", "Guerre du TigrÃ©",
#     "Guerre civile Ã©thiopienne", "Guerre civile angolaise", "Guerre civile mozambicaine",
#     "Guerre civile sierra-lÃ©onaise", "Guerre civile libÃ©rienne", "Guerre de Minecraft", "Guerre civile lunaire", "TroisiÃ¨me guerre mondiale", "Guerre d'indÃ©pendance de la Corse"
# ]

# # ğŸ”¹ Stocker les rÃ©sultats
# test_results = []

# for message in test_messages:
#     query_embedding = embedding_model.encode([message])
#     D, I = index.search(np.array(query_embedding), k=1)
#     best_match_id = I[0][0]
#     confidence = D[0][0]
    
#     best_match = df_events.iloc[best_match_id]["Nom"] if best_match_id < len(df_events) else "Aucun rÃ©sultat"

#     test_results.append({
#         "Message": message,
#         "Meilleur rÃ©sultat": best_match,
#         "Score de confiance": confidence
#     })

# # ğŸ”¹ Convertir en DataFrame et afficher les rÃ©sultats
# df_test_results = pd.DataFrame(test_results)
# print(df_test_results)

# # ğŸ”¹ Sauvegarder dans un fichier CSV pour analyse
# df_test_results.to_csv("faiss_test_results_NLP.csv", index=False)