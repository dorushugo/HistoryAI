from fastapi import FastAPI
from pydantic import BaseModel
import ollama
import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import joblib

# üîπ Charger le mod√®le et le vectorizer entra√Æn√©s
vectorizer = joblib.load("vectorizer.pkl")
classifier = joblib.load("intent_classifier.pkl")

def detect_intent(message):
    """ D√©tecte l‚Äôintention de l‚Äôutilisateur """
    X = vectorizer.transform([message])
    print(classifier.predict(X)[0])
    return classifier.predict(X)[0]
    
app = FastAPI()

# üìå D√©finir le mod√®le Pydantic pour la requ√™te
class ChatRequest(BaseModel):
    user_id: str
    message: str

# üîπ Charger FAISS et le mod√®le d'embeddings
index = faiss.read_index("data/guerres_faiss_NLP.index")
df_events = pd.read_pickle("data/guerres_mapping_NLP.pkl")
# embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
embedding_model = SentenceTransformer('Lajavaness/bilingual-document-embedding', trust_remote_code=True)

quiz_sessions = {}
conversation_memory = {}

def generate_quiz(topic):
    """ G√©n√®re un quizz √† partir de la base de donn√©es vectorielle ou indique que le sujet est inconnu """
    global quiz_sessions
    global conversation_memory
    user_id ="123"

        # ‚úÖ Cr√©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # ‚úÖ V√©rifier que chaque √©l√©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("‚ö†Ô∏è Erreur : `conversation_memory` contient des donn√©es incorrectes ! R√©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # üîπ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)

    # üîé 1Ô∏è‚É£ Recherche du sujet dans la base vectorielle FAISS
    query_embedding = embedding_model.encode([topic])
    D, I = index.search(np.array(query_embedding), k=1)  # R√©cup√©rer le meilleur r√©sultat
    best_match_id = I[0][0]
    confidence = D[0][0]

    SEUIL_CONFIANCE_BAS = 5  # En dessous, on dit qu'on ne conna√Æt pas
    SEUIL_CONFIANCE_LLM = 2  # Entre 5 et 10, on peut utiliser le LLM pour compl√©ter

    if confidence < SEUIL_CONFIANCE_LLM:
        # üìå R√©cup√©rer les informations du sujet
        event = df_events.iloc[best_match_id]
        contexte = (
            f"üìú **{event['Nom']}**\n"
            f"üìÖ **Date** : {event['Date']}\n"
            f"üìç **Lieu** : {event['Lieu']}\n"
            f"üìù **R√©sum√©** : {event['R√©sum√©']}\n"
        )
        # üéØ 2Ô∏è‚É£ G√©n√©rer un quizz bas√© sur ces informations
        llm_prompt = f"""
        Voici l'historique de la conversation avec l'utilisateur :
        {conversation_text}
        
        Mets-toi dans la peau d'un assistant pour les r√©visions.
        Utilise les informations suivantes pour cr√©er un quizz sur {topic}.
        Donne 5 questions √† choix multiples pertinentes sans donner les r√©ponses.

        **Informations :**
        {contexte}
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
        llm_quiz = response["message"]["content"]

        # üìå Sauvegarde des questions g√©n√©r√©es
        quiz_sessions[user_id] = [(q.strip(), "??") for q in llm_quiz.split("\n") if q.strip()]
        print('üìã Quizz g√©n√©r√© avec les donn√©es disponibles :\n\n')
        return f"{llm_quiz}"

    elif confidence > SEUIL_CONFIANCE_LLM and confidence < SEUIL_CONFIANCE_BAS:
        # üìå 3Ô∏è‚É£ Confiance trop faible ‚Üí On utilise le LLM sans source v√©rifi√©e
        llm_prompt = f"""
        Mets-toi dans la peau d'un assistant pour les r√©visions.
        G√©n√®re un quizz sur {topic}.
        Donne 5 questions √† choix multiples pertinentes sans donner les r√©ponses.
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
        llm_quiz = response["message"]["content"]

        # üìå Sauvegarde temporaire des questions g√©n√©r√©es (sans r√©ponses)
        quiz_sessions[user_id] = [(q.strip(), "??") for q in llm_quiz.split("\n") if q.strip()]
        print("üìã Quizz g√©n√©r√© par l'IA (donn√©es incertaines) :\n\n")
        return f"{llm_quiz}"

    else:
        # üìå 4Ô∏è‚É£ Confiance trop basse ‚Üí On refuse de r√©pondre
        return "‚ö†Ô∏è D√©sol√©, je n‚Äôai pas assez d‚Äôinformations pour g√©n√©rer un quizz sur ce sujet."

def provide_quiz_answers(user_id):
    """ Renvoie les r√©ponses du dernier quizz demand√© par l'utilisateur, si disponible """

    global quiz_sessions  # üìå Permet d'acc√©der √† la variable globale
    global conversation_memory
        # ‚úÖ Cr√©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # ‚úÖ V√©rifier que chaque √©l√©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("‚ö†Ô∏è Erreur : `conversation_memory` contient des donn√©es incorrectes ! R√©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # üîπ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)

    if user_id not in quiz_sessions:
        return "‚ö†Ô∏è Aucun quizz en cours. Demandez-moi d‚Äôen g√©n√©rer un !"

    # üìå R√©cup√©rer les questions stock√©es
    questions = quiz_sessions[user_id]

    # üéØ V√©rifier si on a d√©j√† les r√©ponses
    if "??" not in [answer for _, answer in questions]:
        return "\n".join([f"‚úÖ {q} : **{a}**" for q, a in questions])

    # üéØ Si les r√©ponses sont inconnues, demander au LLM
    print("‚ö†Ô∏è R√©ponses manquantes, g√©n√©ration par LLM...")

    llm_prompt = f"""
    Voici l'historique de la conversation avec l'utilisateur :
    {conversation_text}

    Voici un quizz d'histoire :
    {chr(10).join([q for q, _ in questions])}

    Donne une r√©ponse pr√©cise pour chaque question sous forme de liste num√©rot√©e.
    """

    response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
    llm_answers = response["message"]["content"].split("\n")

    # üìå Mettre √† jour les r√©ponses dans la session
    for i, (q, _) in enumerate(questions):
        if i < len(llm_answers):
            quiz_sessions[user_id][i] = (q, llm_answers[i].strip())
    print("üìã R√©ponses du quizz :\n\n")
    return f"{chr(10).join(llm_answers)}"

def generate_summary(topic):
    """ G√©n√®re un r√©sum√© simple et p√©dagogique pour aider un √©l√®ve √† r√©viser """
    global conversation_memory
    user_id ="123"

    # ‚úÖ Cr√©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # ‚úÖ V√©rifier que chaque √©l√©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("‚ö†Ô∏è Erreur : `conversation_memory` contient des donn√©es incorrectes ! R√©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # üîπ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)
    # üîé 1Ô∏è‚É£ Recherche de l'√©v√©nement dans FAISS
    query_embedding = embedding_model.encode([topic])
    D, I = index.search(np.array(query_embedding), k=1)
    best_match_id = I[0][0]
    confidence = D[0][0]
    print(confidence)
    SEUIL_CONFIANCE_BAS = 5

    if confidence < SEUIL_CONFIANCE_BAS:
        event = df_events.iloc[best_match_id]
        contexte = f"üìú **{event['Nom']}**\nüìÖ **Date** : {event['Date']}\nüìç **Lieu** : {event['Lieu']}\nüìù **R√©sum√©** : {event['R√©sum√©']}"

        # üéØ Demander au LLM un r√©sum√© pour un √©l√®ve
        llm_prompt = f"""
        Voici l'historique de la conversation avec l'utilisateur :
        {conversation_text}

        Tu es un professeur d'histoire. R√©dige un r√©sum√© p√©dagogique sur {event['Nom']} destin√© √† un √©l√®ve de lyc√©e. 
        Utilise ces informations :
        {contexte}

        Explique de mani√®re simple et claire, en 150 mots maximum.
        Ne parle pas √† l'√©l√®ve, donne-lui simplement le r√©sum√©
        """

        response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
        print(f"üìú **R√©sum√© p√©dagogique de {event['Nom']}** :\n\n")
        return f"{response['message']['content']}"

    return "‚ö†Ô∏è D√©sol√©, je n‚Äôai pas assez d‚Äôinformations pour g√©n√©rer un r√©sum√© sur ce sujet."

def search_history(message):
    """ Recherche un √©v√©nement historique et l'explique en d√©tail pour un √©l√®ve """
    global conversation_memory
    user_id ="123"

    # ‚úÖ Cr√©er un texte avec les derniers messages
    conversation_history = conversation_memory[user_id]
    
    # ‚úÖ V√©rifier que chaque √©l√©ment est bien un dictionnaire
    if not all(isinstance(msg, dict) for msg in conversation_history):
        print("‚ö†Ô∏è Erreur : `conversation_memory` contient des donn√©es incorrectes ! R√©initialisation...")
        conversation_memory[user_id] = []
        conversation_history = []

    # üîπ Construire la conversation sous forme de texte
    conversation_text = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])
    print(conversation_text)

    # üîé G√©n√©rer l'embedding de la requ√™te
    query_embedding = embedding_model.encode([message])
    D, I = index.search(np.array(query_embedding), k=1)
    best_match_id = I[0][0]
    confidence = D[0][0]
    print(confidence)
    SEUIL_CONFIANCE_BAS = 5

    if confidence > SEUIL_CONFIANCE_BAS:
        return "‚ö†Ô∏è D√©sol√©, je n‚Äôai pas trouv√© d‚Äôinformations pr√©cises sur ce sujet."

    event = df_events.iloc[best_match_id]
    contexte = f"üìú **{event['Nom']}**\nüìÖ **Date** : {event['Date']}\nüìç **Lieu** : {event['Lieu']}\nüìù **R√©sum√©** : {event['R√©sum√©']}"
    print(contexte)
    # üéØ Demander au LLM d'expliquer le sujet en d√©tail
    llm_prompt = f"""
    Voici l'historique de la conversation avec l'utilisateur :
    {conversation_text}

    Explique √† un √©l√®ve de lyc√©e l'√©v√©nement historique suivant :
    {contexte}

    R√©dige une explication d√©taill√©e et p√©dagogique en 300 mots maximum.
    Ne parle pas √† l'√©l√®ve, donne-lui simplement l'explication d√©taill√©e

    """
    # Structure la r√©ponse en introduction, d√©veloppement et conclusion.

    response = ollama.chat(model="llama3.2", messages=[{"role": "user", "content": llm_prompt}])
    print(f"üìú **Explication d√©taill√©e de {event['Nom']}** :\n\n")
    return f"{response['message']['content']}"

@app.post("/chat")
def chat(request: ChatRequest):
    """ G√®re la conversation et d√©tecte l‚Äôintention automatiquement """

    user_id = request.user_id
    message = request.message.lower()

    # üéØ 1Ô∏è‚É£ V√©rifier que l'utilisateur a une liste d'historique
    if user_id not in conversation_memory:
        conversation_memory[user_id] = []  # üìå Cr√©er une liste vide pour ce user
    conversation_history = conversation_memory[user_id]

    # ‚úÖ Enregistrer le message de l'utilisateur dans l'historique AVANT toute analyse
    conversation_history.append({"role": "user", "content": message})

    # üéØ 1Ô∏è‚É£ D√©tecter l‚Äôintention de la requ√™te
    intent = detect_intent(message)

    print(f"üì© Message re√ßu : {message} ‚Üí Intention d√©tect√©e : {intent}")

    # üéØ 2Ô∏è‚É£ Rediriger vers la bonne fonction
    if intent == "quizz":
        response_text = generate_quiz(message)
    elif intent == "quizz_reponses":
        response_text = provide_quiz_answers(user_id)
    elif intent == "r√©sum√©":
        response_text = generate_summary(message)
    elif intent == "d√©tail":
        response_text = search_history(message)
    else:
        response_text = "‚ö†Ô∏è Je ne suis pas s√ªr de comprendre. Peux-tu reformuler ?"
    # üìå Stocker la r√©ponse dans la m√©moire
    conversation_history.append({"role": "assistant", "content": response_text})
    return {"response": response_text}





# import numpy as np
# import pandas as pd

# # üîπ Charger FAISS et le mod√®le d'embeddings
# index = faiss.read_index("data/guerres_faiss_NLP.index")
# df_events = pd.read_pickle("data/guerres_mapping_NLP.pkl")
# embedding_model = SentenceTransformer('Lajavaness/bilingual-document-embedding', trust_remote_code=True)

# # üîπ Liste de requ√™tes historiques
# test_messages = [
#     "Guerre de la Ligue de Cambrai", "Guerre de Trente Ans", "Guerre de D√©volution",
#     "Guerre de Hollande", "Guerre de la Ligue d'Augsbourg", "Guerre de Succession d'Espagne",
#     "Guerre de la Quadruple-Alliance", "Guerre de Succession de Pologne", "Guerre de Succession d'Autriche",
#     "Guerre de Sept Ans", "Guerre d'Ind√©pendance des √âtats-Unis", "Guerres de la R√©volution fran√ßaise",
#     "Guerres napol√©oniennes", "Guerre de 1812", "Guerre d'ind√©pendance grecque",
#     "Guerre de Crim√©e", "Guerre de S√©cession", "Guerre franco-prussienne",
#     "Guerre des Boers", "Premi√®re Guerre mondiale", "Guerre civile russe",
#     "Guerre d'ind√©pendance irlandaise", "Guerre civile chinoise", "Guerre d'Espagne",
#     "Seconde Guerre mondiale", "Guerre de Cor√©e", "Guerre d'Alg√©rie",
#     "Guerre du Vietnam", "Guerre du Yom Kippour", "Guerre d'Afghanistan",
#     "Guerre Iran-Irak", "Guerre du Golfe", "Guerre de Bosnie",
#     "Guerre du Kosovo", "Guerre d'Irak", "Guerre civile syrienne",
#     "Guerre du Donbass", "Guerre du Haut-Karabagh", "Guerre civile libyenne",
#     "Guerre civile y√©m√©nite", "Guerre du Mali", "Guerre civile centrafricaine",
#     "Guerre du Darfour", "Guerre civile sud-soudanaise", "Guerre du Tigr√©",
#     "Guerre civile √©thiopienne", "Guerre civile angolaise", "Guerre civile mozambicaine",
#     "Guerre civile sierra-l√©onaise", "Guerre civile lib√©rienne", "Guerre de Minecraft", "Guerre civile lunaire", "Troisi√®me guerre mondiale", "Guerre d'ind√©pendance de la Corse"
# ]

# # üîπ Stocker les r√©sultats
# test_results = []

# for message in test_messages:
#     query_embedding = embedding_model.encode([message])
#     D, I = index.search(np.array(query_embedding), k=1)
#     best_match_id = I[0][0]
#     confidence = D[0][0]
    
#     best_match = df_events.iloc[best_match_id]["Nom"] if best_match_id < len(df_events) else "Aucun r√©sultat"

#     test_results.append({
#         "Message": message,
#         "Meilleur r√©sultat": best_match,
#         "Score de confiance": confidence
#     })

# # üîπ Convertir en DataFrame et afficher les r√©sultats
# df_test_results = pd.DataFrame(test_results)
# print(df_test_results)

# # üîπ Sauvegarder dans un fichier CSV pour analyse
# df_test_results.to_csv("faiss_test_results_NLP.csv", index=False)